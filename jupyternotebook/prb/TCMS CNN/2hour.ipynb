{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# Parameter windowing\n",
    "input_width = 60*2\n",
    "label_width = 60\n",
    "shift = label_width\n",
    "total_window_size = input_width + shift\n",
    "OUT_STEPS = label_width\n",
    "# Definisikan irisan untuk input dan label\n",
    "input_slice = slice(0, input_width)\n",
    "label_start = total_window_size - label_width\n",
    "labels_slice = slice(label_start, None)\n",
    "train_df = None\n",
    "output_selected=['RRU.PrbUsedDl']\n",
    "train_name_cells=['S1/B2/C1']\n",
    "test_name_cells=['S7/B2/C1']\n",
    "file_path=\"D:\\\\KULIAH\\\\teep\\\\AI\\\\dataset\\\\08_01_2024\\\\CellReports.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_coding(timestamps, max_values):\n",
    "    sin_features = np.sin(2 * np.pi * timestamps / max_values)\n",
    "    cos_features = np.cos(2 * np.pi * timestamps / max_values)\n",
    "    periodic_features = np.concatenate([sin_features, cos_features], axis=-1)\n",
    "    return periodic_features\n",
    "\n",
    "def  preprocess_data(file_path):\n",
    "    df_1=pd.read_csv(file_path)\n",
    "    convert_time=pd.to_datetime(df_1['timestamp'], unit='ms',origin='unix')\n",
    "    df_1.insert(df_1.columns.get_loc('timestamp') + 1, 'datetime_column', convert_time)\n",
    "    df_1.insert(df_1.columns.get_loc('datetime_column') + 2, 'hour', df_1['datetime_column'].dt.hour+df_1['datetime_column'].dt.minute/60)\n",
    "    df_1.set_index('datetime_column', inplace=True)\n",
    "    df_1.drop(columns=['timestamp'], inplace=True)\n",
    "    df_1['sin_time'] = np.sin(df_1['hour'] * (2 * np.pi / 24))\n",
    "    df_1['cos_time'] = np.cos(df_1['hour']* (2 * np.pi / 24))\n",
    "    seleted_columns = ['Viavi.Cell.Name','RRU.PrbUsedDl', 'sin_time', 'cos_time'] \n",
    "    df_2= df_1[seleted_columns].copy()\n",
    "    cell_name= train_name_cells+test_name_cells\n",
    "    df= df_2[df_2['Viavi.Cell.Name'].isin(cell_name)].copy()\n",
    "    \n",
    "    return df, cell_name\n",
    "\n",
    "def make_windows(data_x,data_y, total_window_size, input_slice, labels_slice):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data_x) - total_window_size + 1):\n",
    "        window_x = data_x[i:i+total_window_size]\n",
    "        x.append(window_x[input_slice])\n",
    "\n",
    "    for i in range(len(data_y) - total_window_size + 1):\n",
    "        window_y= data_y[i:i+total_window_size]\n",
    "        y.append(window_y[labels_slice])\n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "def standardize_data(data, train_df, isoutput=True, column_output=output_selected):\n",
    "    if isoutput:\n",
    "        median = train_df[column_output].median().values\n",
    "        q1 = train_df[column_output].quantile(0.25).values\n",
    "        q3 = train_df[column_output].quantile(0.75).values\n",
    "    else:\n",
    "        median = train_df.median().values\n",
    "        q1 = train_df.quantile(0.25).values\n",
    "        q3 = train_df.quantile(0.75).values\n",
    "\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Reshape for broadcasting with 2D matrix\n",
    "    median = median.reshape(1, -1)\n",
    "    iqr = iqr.reshape(1, -1)\n",
    "\n",
    "    return (data - median) / iqr\n",
    "\n",
    "def inverse_standardize_data(data, train_df, isoutput=True, column_output=output_selected):\n",
    "    if isoutput:\n",
    "        median = train_df[column_output].median().values\n",
    "        q1 = train_df[column_output].quantile(0.25).values\n",
    "        q3 = train_df[column_output].quantile(0.75).values\n",
    "    else:\n",
    "        median = train_df.median().values\n",
    "        q1 = train_df.quantile(0.25).values\n",
    "        q3 = train_df.quantile(0.75).values\n",
    "\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Reshape for broadcasting with 2D matrix\n",
    "    median = median.reshape(1, -1)\n",
    "    iqr = iqr.reshape(1, -1)\n",
    "\n",
    "    return data * iqr + median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GAME\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "d:\\GAME\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import time\n",
    "import tensorflow_addons as tfa\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='tensorflow_addons')\n",
    "\n",
    "# Define the PinballLoss function\n",
    "pinball_loss = tfa.losses.PinballLoss(tau=0.5, reduction=tf.keras.losses.Reduction.AUTO, name='pinball_loss')\n",
    "\n",
    "def tensorflow_cnn(X_train_scaled, Y_train_scaled, X_validation_scaled, Y_validation_scaled, \n",
    "                    learning_rate, target_error, max_epochs, max_sampel_batch,\n",
    "                    patience, save_best_model_path, validation_data=False, load_model=None, out_steps=OUT_STEPS):\n",
    "    global model\n",
    "\n",
    "    class MAEStopCallback(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, threshold):\n",
    "            super(MAEStopCallback, self).__init__()\n",
    "            self.threshold = threshold\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if logs['mae'] < self.threshold:\n",
    "                print(f\"\\nMAE reached below {self.threshold}. Stopping training.\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "                \n",
    "    input_width = X_train_scaled.shape[1]\n",
    "    CONV_WIDTH = input_width # Define the width of the convolutional window\n",
    "    num_features = X_train_scaled.shape[2]\n",
    "    num_output = Y_train_scaled.shape[2]\n",
    "    out_steps = out_steps\n",
    "\n",
    "    if load_model is None:\n",
    "        print(\"Create new model\")\n",
    "\n",
    "        # Define the model\n",
    "        inputs = tf.keras.Input(shape=(input_width, num_features))\n",
    "\n",
    "        # Initial Conv1D layer\n",
    "        layer_first = tf.keras.layers.Conv1D(filters=64, kernel_size=8, dilation_rate=1, \n",
    "                                            activation='relu', padding='same')(inputs)\n",
    "\n",
    "        # Residual connection initialization\n",
    "        residual_before = tf.keras.layers.Conv1D(filters=24, kernel_size=8, dilation_rate=1, activation='relu', padding='same')(layer_first)\n",
    "\n",
    "        # Residual block with multiple dilations\n",
    "        for _ in range(8):\n",
    "            for dilation in (1, 2, 4):\n",
    "                residual = tf.keras.layers.Conv1D(filters=24, kernel_size=8, dilation_rate=dilation, activation='relu', padding='same')(residual_before)\n",
    "                residual = tf.keras.layers.BatchNormalization()(residual)\n",
    "                residual = tf.keras.layers.Dropout(0.05)(residual)\n",
    "                residual = tf.keras.layers.Add()([residual_before, residual])\n",
    "                residual_before = residual\n",
    "\n",
    "        # Output layer\n",
    "        last_cnn= tf.keras.layers.Conv1D(filters=60, kernel_size=input_width, dilation_rate=1, activation='linear', name='last_cnn')(residual_before)\n",
    "        output = tf.keras.layers.Dense(out_steps * num_output, name='output_layer')(last_cnn)\n",
    "        output=tf.keras.layers.Reshape([out_steps, num_output])(output)\n",
    "        # Define the model\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "        model.summary()\n",
    "    else:\n",
    "        print(\"Load model\")\n",
    "        model = tf.keras.models.load_model(load_model)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,amsgrad=True)\n",
    "    model.compile(optimizer=optimizer, \n",
    "              loss=pinball_loss, \n",
    "              metrics=['mse', 'mae', 'mape', tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "    mae_stop_callback = MAEStopCallback(threshold=target_error)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        save_best_model_path,\n",
    "        monitor='val_loss',     \n",
    "        mode='min',         \n",
    "        save_best_only=True, \n",
    "        verbose=1            \n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_loss',     \n",
    "        mode='min',         \n",
    "        patience=patience,    \n",
    "        restore_best_weights=True,\n",
    "        verbose=1            \n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.3, patience=int(5), min_lr=0.00001, verbose=1)\n",
    "\n",
    "    time_start = time.time()\n",
    "    if validation_data:\n",
    "        model.fit(X_train_scaled, Y_train_scaled, epochs=max_epochs, batch_size=max_sampel_batch,  \n",
    "                  callbacks=[mae_stop_callback, checkpoint_callback, early_stopping_callback, reduce_lr], \n",
    "                  validation_data=(X_validation_scaled, Y_validation_scaled), validation_batch_size=max_sampel_batch)\n",
    "    else:\n",
    "        model.fit(X_train_scaled, Y_train_scaled, epochs=max_epochs, batch_size=max_sampel_batch, \n",
    "                  callbacks=[mae_stop_callback, checkpoint_callback, early_stopping_callback, reduce_lr])\n",
    "    \n",
    "    print(\"time computation seconds: \", time.time() - time_start)\n",
    "    \n",
    "    loss, MSE, MAE, RMSE, MAPE = model.evaluate(X_train_scaled, Y_train_scaled)\n",
    "    print(\"loss: \", loss, \"MSE: \", MSE, \"MAE: \", MAE, \"RMSE: \", RMSE, \"MAPE: \", MAPE)\n",
    "    \n",
    "    return model, loss, MSE, MAE, RMSE, MAPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_program(train_df,val_df, index_cell, name_file, name_file_before):\n",
    "    train_scaled = standardize_data(train_df, isoutput=False, train_df=train_df)\n",
    "    val_scaled= standardize_data(val_df, isoutput=False, train_df=train_df)\n",
    "    # Membuat windowed dataset untuk set pelatihan, validasi, dan pengujian\n",
    "\n",
    "    x_train_scaled, y_train_scaled = make_windows(train_scaled.to_numpy(), train_scaled[output_selected].to_numpy(),total_window_size, input_slice, labels_slice)\n",
    "    x_val_scaled, y_val_scaled = make_windows(val_scaled.to_numpy(), val_scaled[output_selected].to_numpy(),total_window_size, input_slice, labels_slice)\n",
    "    model, loss, MSE, MAE, RMSE,  MAPE  = tensorflow_cnn(x_train_scaled, y_train_scaled, x_val_scaled, y_val_scaled,\n",
    "                                                        learning_rate=0.01, target_error=0.001,  max_epochs=100, max_sampel_batch=128, \n",
    "                                                        patience=30,  save_best_model_path = name_file, \n",
    "                                                        validation_data=True, load_model=None, out_steps=OUT_STEPS)\n",
    "    model.save(\"TCSM_CNN.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "name_file:  2hour_cekpoint_1.hdf5\n",
      "name_file_before:  2hour_cekpoint_0.hdf5\n",
      "Cell Name:  S1/B2/C1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RRU.PrbUsedDl</th>\n",
       "      <th>sin_time</th>\n",
       "      <th>cos_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime_column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-08-01 00:00:00</th>\n",
       "      <td>35.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-01 00:01:00</th>\n",
       "      <td>20.116667</td>\n",
       "      <td>0.004363</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-01 00:02:00</th>\n",
       "      <td>30.800000</td>\n",
       "      <td>0.008727</td>\n",
       "      <td>0.999962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-01 00:03:00</th>\n",
       "      <td>34.450000</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>0.999914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-01 00:04:00</th>\n",
       "      <td>25.900000</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.999848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-08 23:55:00</th>\n",
       "      <td>25.116667</td>\n",
       "      <td>-0.021815</td>\n",
       "      <td>0.999762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-08 23:56:00</th>\n",
       "      <td>24.666667</td>\n",
       "      <td>-0.017452</td>\n",
       "      <td>0.999848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-08 23:57:00</th>\n",
       "      <td>59.133333</td>\n",
       "      <td>-0.013090</td>\n",
       "      <td>0.999914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-08 23:58:00</th>\n",
       "      <td>6.583333</td>\n",
       "      <td>-0.008727</td>\n",
       "      <td>0.999962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-08 23:59:00</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>-0.004363</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11520 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     RRU.PrbUsedDl  sin_time  cos_time\n",
       "datetime_column                                       \n",
       "2024-08-01 00:00:00      35.200000  0.000000  1.000000\n",
       "2024-08-01 00:01:00      20.116667  0.004363  0.999990\n",
       "2024-08-01 00:02:00      30.800000  0.008727  0.999962\n",
       "2024-08-01 00:03:00      34.450000  0.013090  0.999914\n",
       "2024-08-01 00:04:00      25.900000  0.017452  0.999848\n",
       "...                            ...       ...       ...\n",
       "2024-08-08 23:55:00      25.116667 -0.021815  0.999762\n",
       "2024-08-08 23:56:00      24.666667 -0.017452  0.999848\n",
       "2024-08-08 23:57:00      59.133333 -0.013090  0.999914\n",
       "2024-08-08 23:58:00       6.583333 -0.008727  0.999962\n",
       "2024-08-08 23:59:00       5.000000 -0.004363  0.999990\n",
       "\n",
       "[11520 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new model\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 120, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 120, 64)      1600        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 120, 24)      12312       ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 120, 24)      4632        ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 120, 24)     96          ['conv1d_2[0][0]']               \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 120, 24)      0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 120, 24)      0           ['conv1d_1[0][0]',               \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 120, 24)      4632        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 120, 24)     96          ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 120, 24)      0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 120, 24)      0           ['add[0][0]',                    \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 120, 24)      4632        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 120, 24)     96          ['conv1d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 120, 24)      0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 120, 24)      0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 120, 24)      4632        ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 120, 24)     96          ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 120, 24)      0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 120, 24)      0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 120, 24)      4632        ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 120, 24)     96          ['conv1d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 120, 24)      0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 120, 24)      0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 120, 24)      4632        ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 120, 24)     96          ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 120, 24)      0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 120, 24)      0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 120, 24)      4632        ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 120, 24)     96          ['conv1d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 120, 24)      0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 120, 24)      0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 120, 24)      4632        ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 120, 24)     96          ['conv1d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 120, 24)      0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 120, 24)      0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 120, 24)      4632        ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 120, 24)     96          ['conv1d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 120, 24)      0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 120, 24)      0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 120, 24)      4632        ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 120, 24)     96          ['conv1d_11[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 120, 24)      0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 120, 24)      0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 120, 24)      4632        ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 120, 24)     96          ['conv1d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 120, 24)      0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 120, 24)      0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 120, 24)      4632        ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 120, 24)     96          ['conv1d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 120, 24)      0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 120, 24)      0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 120, 24)      4632        ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 120, 24)     96          ['conv1d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 120, 24)      0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 120, 24)      0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 120, 24)      4632        ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 120, 24)     96          ['conv1d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 120, 24)      0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 120, 24)      0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 120, 24)      4632        ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 120, 24)     96          ['conv1d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 120, 24)      0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 120, 24)      0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 120, 24)      4632        ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 120, 24)     96          ['conv1d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 120, 24)      0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 120, 24)      0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 120, 24)      4632        ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 120, 24)     96          ['conv1d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 120, 24)      0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 120, 24)      0           ['add_15[0][0]',                 \n",
      "                                                                  'dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 120, 24)      4632        ['add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 120, 24)     96          ['conv1d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 120, 24)      0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 120, 24)      0           ['add_16[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 120, 24)      4632        ['add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 120, 24)     96          ['conv1d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 120, 24)      0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 120, 24)      0           ['add_17[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 120, 24)      4632        ['add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 120, 24)     96          ['conv1d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 120, 24)      0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 120, 24)      0           ['add_18[0][0]',                 \n",
      "                                                                  'dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 120, 24)      4632        ['add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 120, 24)     96          ['conv1d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 120, 24)      0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 120, 24)      0           ['add_19[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 120, 24)      4632        ['add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 120, 24)     96          ['conv1d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 120, 24)      0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 120, 24)      0           ['add_20[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 120, 24)      4632        ['add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 120, 24)     96          ['conv1d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 120, 24)      0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 120, 24)      0           ['add_21[0][0]',                 \n",
      "                                                                  'dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 120, 24)      4632        ['add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 120, 24)     96          ['conv1d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 120, 24)      0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 120, 24)      0           ['add_22[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " last_cnn (Conv1D)              (None, 1, 60)        172860      ['add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 1, 60)        3660        ['last_cnn[0][0]']               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 60, 1)        0           ['output_layer[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 303,904\n",
      "Trainable params: 302,752\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.8886 - mse: 69.2793 - mae: 3.7773 - mape: 1644067.5000 - rmse: 8.3234\n",
      "Epoch 1: val_loss improved from inf to 5.24020, saving model to 2hour_cekpoint_1.hdf5\n",
      "89/89 [==============================] - 14s 54ms/step - loss: 1.8886 - mse: 69.2793 - mae: 3.7773 - mape: 1644067.5000 - rmse: 8.3234 - val_loss: 5.2402 - val_mse: 269.3262 - val_mae: 10.4804 - val_mape: 383264.2812 - val_rmse: 16.4112 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.3446 - mse: 0.7642 - mae: 0.6891 - mape: 237654.7500 - rmse: 0.8742\n",
      "Epoch 2: val_loss improved from 5.24020 to 0.43091, saving model to 2hour_cekpoint_1.hdf5\n",
      "89/89 [==============================] - 4s 41ms/step - loss: 0.3446 - mse: 0.7642 - mae: 0.6891 - mape: 237654.7500 - rmse: 0.8742 - val_loss: 0.4309 - val_mse: 1.4846 - val_mae: 0.8618 - val_mape: 86578.6641 - val_rmse: 1.2184 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.2739 - mse: 0.4729 - mae: 0.5479 - mape: 143736.3438 - rmse: 0.6877\n",
      "Epoch 3: val_loss improved from 0.43091 to 0.33440, saving model to 2hour_cekpoint_1.hdf5\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2739 - mse: 0.4729 - mae: 0.5479 - mape: 143736.3438 - rmse: 0.6877 - val_loss: 0.3344 - val_mse: 0.6531 - val_mae: 0.6688 - val_mape: 58536.2930 - val_rmse: 0.8082 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.2535 - mse: 0.4044 - mae: 0.5071 - mape: 130677.7500 - rmse: 0.6359\n",
      "Epoch 4: val_loss improved from 0.33440 to 0.32989, saving model to 2hour_cekpoint_1.hdf5\n",
      "89/89 [==============================] - 4s 41ms/step - loss: 0.2535 - mse: 0.4044 - mae: 0.5071 - mape: 130677.7500 - rmse: 0.6359 - val_loss: 0.3299 - val_mse: 0.6302 - val_mae: 0.6598 - val_mape: 61565.7227 - val_rmse: 0.7938 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.2404 - mse: 0.3646 - mae: 0.4808 - mape: 113979.9922 - rmse: 0.6038\n",
      "Epoch 5: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 41ms/step - loss: 0.2404 - mse: 0.3646 - mae: 0.4808 - mape: 113979.9922 - rmse: 0.6038 - val_loss: 0.3501 - val_mse: 0.6839 - val_mae: 0.7003 - val_mape: 69092.5469 - val_rmse: 0.8270 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.2324 - mse: 0.3415 - mae: 0.4648 - mape: 103048.5469 - rmse: 0.5843\n",
      "Epoch 6: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 41ms/step - loss: 0.2324 - mse: 0.3415 - mae: 0.4648 - mape: 103048.5469 - rmse: 0.5843 - val_loss: 0.3793 - val_mse: 0.8025 - val_mae: 0.7586 - val_mape: 59085.0625 - val_rmse: 0.8958 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.2264 - mse: 0.3255 - mae: 0.4528 - mape: 96249.9766 - rmse: 0.5705\n",
      "Epoch 7: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2264 - mse: 0.3255 - mae: 0.4528 - mape: 96249.9766 - rmse: 0.5705 - val_loss: 0.3743 - val_mse: 0.7799 - val_mae: 0.7485 - val_mape: 59885.0234 - val_rmse: 0.8831 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.2201 - mse: 0.3089 - mae: 0.4402 - mape: 95929.8750 - rmse: 0.5558\n",
      "Epoch 8: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2201 - mse: 0.3089 - mae: 0.4402 - mape: 95929.8750 - rmse: 0.5558 - val_loss: 0.3786 - val_mse: 0.8130 - val_mae: 0.7572 - val_mape: 53343.4453 - val_rmse: 0.9017 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.2155 - mse: 0.2976 - mae: 0.4310 - mape: 87184.8281 - rmse: 0.5455\n",
      "Epoch 9: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2155 - mse: 0.2976 - mae: 0.4310 - mape: 87184.8281 - rmse: 0.5455 - val_loss: 0.3488 - val_mse: 0.7024 - val_mae: 0.6976 - val_mape: 55467.8555 - val_rmse: 0.8381 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.2124 - mse: 0.2901 - mae: 0.4248 - mape: 88830.1250 - rmse: 0.5386\n",
      "Epoch 10: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2124 - mse: 0.2901 - mae: 0.4248 - mape: 88830.1250 - rmse: 0.5386 - val_loss: 0.3962 - val_mse: 0.8701 - val_mae: 0.7924 - val_mape: 50253.5781 - val_rmse: 0.9328 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.2097 - mse: 0.2843 - mae: 0.4194 - mape: 83632.6953 - rmse: 0.5332\n",
      "Epoch 11: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2097 - mse: 0.2843 - mae: 0.4194 - mape: 83632.6953 - rmse: 0.5332 - val_loss: 0.3784 - val_mse: 0.8004 - val_mae: 0.7569 - val_mape: 54091.9766 - val_rmse: 0.8947 - lr: 0.0100\n",
      "Epoch 12/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.2069 - mse: 0.2778 - mae: 0.4137 - mape: 79967.4609 - rmse: 0.5271\n",
      "Epoch 12: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2069 - mse: 0.2778 - mae: 0.4137 - mape: 79967.4609 - rmse: 0.5271 - val_loss: 0.3700 - val_mse: 0.7721 - val_mae: 0.7401 - val_mape: 58856.7422 - val_rmse: 0.8787 - lr: 0.0100\n",
      "Epoch 13/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.2042 - mse: 0.2718 - mae: 0.4083 - mape: 78043.4297 - rmse: 0.5213\n",
      "Epoch 13: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 41ms/step - loss: 0.2042 - mse: 0.2718 - mae: 0.4083 - mape: 78043.4297 - rmse: 0.5213 - val_loss: 0.3795 - val_mse: 0.8113 - val_mae: 0.7590 - val_mape: 65403.8359 - val_rmse: 0.9007 - lr: 0.0100\n",
      "Epoch 14/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.2022 - mse: 0.2679 - mae: 0.4044 - mape: 73254.1875 - rmse: 0.5176\n",
      "Epoch 14: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 41ms/step - loss: 0.2022 - mse: 0.2679 - mae: 0.4044 - mape: 73254.1875 - rmse: 0.5176 - val_loss: 0.3572 - val_mse: 0.7548 - val_mae: 0.7144 - val_mape: 67799.0547 - val_rmse: 0.8688 - lr: 0.0100\n",
      "Epoch 15/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.2010 - mse: 0.2657 - mae: 0.4021 - mape: 74105.1875 - rmse: 0.5155\n",
      "Epoch 15: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 41ms/step - loss: 0.2010 - mse: 0.2657 - mae: 0.4021 - mape: 74105.1875 - rmse: 0.5155 - val_loss: 0.3955 - val_mse: 0.9024 - val_mae: 0.7910 - val_mape: 53214.1523 - val_rmse: 0.9499 - lr: 0.0100\n",
      "Epoch 16/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1993 - mse: 0.2620 - mae: 0.3987 - mape: 77076.0312 - rmse: 0.5118\n",
      "Epoch 16: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.1993 - mse: 0.2620 - mae: 0.3987 - mape: 77076.0312 - rmse: 0.5118 - val_loss: 0.3495 - val_mse: 0.7188 - val_mae: 0.6990 - val_mape: 63711.8594 - val_rmse: 0.8478 - lr: 0.0100\n",
      "Epoch 17/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1973 - mse: 0.2577 - mae: 0.3947 - mape: 78795.6016 - rmse: 0.5077\n",
      "Epoch 17: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.1973 - mse: 0.2577 - mae: 0.3947 - mape: 78795.6016 - rmse: 0.5077 - val_loss: 0.3308 - val_mse: 0.6558 - val_mae: 0.6616 - val_mape: 52199.6992 - val_rmse: 0.8098 - lr: 0.0100\n",
      "Epoch 18/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1966 - mse: 0.2563 - mae: 0.3933 - mape: 73225.4062 - rmse: 0.5062\n",
      "Epoch 18: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 41ms/step - loss: 0.1966 - mse: 0.2563 - mae: 0.3933 - mape: 73225.4062 - rmse: 0.5062 - val_loss: 0.3379 - val_mse: 0.6650 - val_mae: 0.6758 - val_mape: 49550.0859 - val_rmse: 0.8155 - lr: 0.0100\n",
      "Epoch 19/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1955 - mse: 0.2541 - mae: 0.3910 - mape: 75437.8438 - rmse: 0.5041\n",
      "Epoch 19: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 41ms/step - loss: 0.1955 - mse: 0.2541 - mae: 0.3910 - mape: 75437.8438 - rmse: 0.5041 - val_loss: 0.3438 - val_mse: 0.7107 - val_mae: 0.6875 - val_mape: 58255.0430 - val_rmse: 0.8430 - lr: 0.0100\n",
      "Epoch 20/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1939 - mse: 0.2505 - mae: 0.3878 - mape: 81451.3750 - rmse: 0.5005\n",
      "Epoch 20: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 43ms/step - loss: 0.1939 - mse: 0.2505 - mae: 0.3878 - mape: 81451.3750 - rmse: 0.5005 - val_loss: 0.3310 - val_mse: 0.6588 - val_mae: 0.6620 - val_mape: 44391.0391 - val_rmse: 0.8117 - lr: 0.0100\n",
      "Epoch 21/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1923 - mse: 0.2470 - mae: 0.3846 - mape: 80070.4922 - rmse: 0.4970\n",
      "Epoch 21: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 43ms/step - loss: 0.1923 - mse: 0.2470 - mae: 0.3846 - mape: 80070.4922 - rmse: 0.4970 - val_loss: 0.3392 - val_mse: 0.6900 - val_mae: 0.6785 - val_mape: 51002.3945 - val_rmse: 0.8307 - lr: 0.0100\n",
      "Epoch 22/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1918 - mse: 0.2461 - mae: 0.3836 - mape: 75341.0000 - rmse: 0.4961\n",
      "Epoch 22: val_loss did not improve from 0.32989\n",
      "89/89 [==============================] - 4s 43ms/step - loss: 0.1918 - mse: 0.2461 - mae: 0.3836 - mape: 75341.0000 - rmse: 0.4961 - val_loss: 0.3512 - val_mse: 0.7270 - val_mae: 0.7024 - val_mape: 63015.6758 - val_rmse: 0.8526 - lr: 0.0100\n",
      "Epoch 23/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1902 - mse: 0.2429 - mae: 0.3803 - mape: 73650.1250 - rmse: 0.4929\n",
      "Epoch 23: val_loss improved from 0.32989 to 0.31695, saving model to 2hour_cekpoint_1.hdf5\n",
      "89/89 [==============================] - 4s 45ms/step - loss: 0.1902 - mse: 0.2429 - mae: 0.3803 - mape: 73650.1250 - rmse: 0.4929 - val_loss: 0.3170 - val_mse: 0.6095 - val_mae: 0.6339 - val_mape: 70839.5547 - val_rmse: 0.7807 - lr: 0.0100\n",
      "Epoch 24/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1890 - mse: 0.2403 - mae: 0.3779 - mape: 74316.7969 - rmse: 0.4902\n",
      "Epoch 24: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1890 - mse: 0.2403 - mae: 0.3779 - mape: 74316.7969 - rmse: 0.4902 - val_loss: 0.3505 - val_mse: 0.7226 - val_mae: 0.7010 - val_mape: 52738.6328 - val_rmse: 0.8501 - lr: 0.0100\n",
      "Epoch 25/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1876 - mse: 0.2376 - mae: 0.3752 - mape: 71169.9922 - rmse: 0.4875\n",
      "Epoch 25: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 43ms/step - loss: 0.1876 - mse: 0.2376 - mae: 0.3752 - mape: 71169.9922 - rmse: 0.4875 - val_loss: 0.3432 - val_mse: 0.7038 - val_mae: 0.6865 - val_mape: 52187.4062 - val_rmse: 0.8389 - lr: 0.0100\n",
      "Epoch 26/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1866 - mse: 0.2353 - mae: 0.3731 - mape: 72223.7891 - rmse: 0.4851\n",
      "Epoch 26: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 43ms/step - loss: 0.1866 - mse: 0.2353 - mae: 0.3731 - mape: 72223.7891 - rmse: 0.4851 - val_loss: 0.3262 - val_mse: 0.6470 - val_mae: 0.6524 - val_mape: 44895.0234 - val_rmse: 0.8044 - lr: 0.0100\n",
      "Epoch 27/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1859 - mse: 0.2340 - mae: 0.3718 - mape: 70743.2422 - rmse: 0.4838\n",
      "Epoch 27: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1859 - mse: 0.2340 - mae: 0.3718 - mape: 70743.2422 - rmse: 0.4838 - val_loss: 0.3432 - val_mse: 0.6970 - val_mae: 0.6864 - val_mape: 47952.9492 - val_rmse: 0.8349 - lr: 0.0100\n",
      "Epoch 28/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1852 - mse: 0.2327 - mae: 0.3705 - mape: 75609.6797 - rmse: 0.4824\n",
      "Epoch 28: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1852 - mse: 0.2327 - mae: 0.3705 - mape: 75609.6797 - rmse: 0.4824 - val_loss: 0.3203 - val_mse: 0.6269 - val_mae: 0.6407 - val_mape: 57323.0117 - val_rmse: 0.7918 - lr: 0.0100\n",
      "Epoch 29/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1845 - mse: 0.2314 - mae: 0.3689 - mape: 73324.5000 - rmse: 0.4810\n",
      "Epoch 29: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1845 - mse: 0.2314 - mae: 0.3689 - mape: 73324.5000 - rmse: 0.4810 - val_loss: 0.3330 - val_mse: 0.6625 - val_mae: 0.6659 - val_mape: 48219.8164 - val_rmse: 0.8140 - lr: 0.0100\n",
      "Epoch 30/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1832 - mse: 0.2287 - mae: 0.3664 - mape: 76912.8672 - rmse: 0.4782\n",
      "Epoch 30: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1832 - mse: 0.2287 - mae: 0.3664 - mape: 76912.8672 - rmse: 0.4782 - val_loss: 0.3344 - val_mse: 0.6654 - val_mae: 0.6689 - val_mape: 42394.3203 - val_rmse: 0.8158 - lr: 0.0100\n",
      "Epoch 31/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1821 - mse: 0.2264 - mae: 0.3641 - mape: 74881.9609 - rmse: 0.4758\n",
      "Epoch 31: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1821 - mse: 0.2264 - mae: 0.3641 - mape: 74881.9609 - rmse: 0.4758 - val_loss: 0.3299 - val_mse: 0.6533 - val_mae: 0.6598 - val_mape: 40918.1992 - val_rmse: 0.8083 - lr: 0.0100\n",
      "Epoch 32/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1816 - mse: 0.2254 - mae: 0.3632 - mape: 73851.0000 - rmse: 0.4748\n",
      "Epoch 32: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 43ms/step - loss: 0.1816 - mse: 0.2254 - mae: 0.3632 - mape: 73851.0000 - rmse: 0.4748 - val_loss: 0.3403 - val_mse: 0.6812 - val_mae: 0.6806 - val_mape: 40940.7031 - val_rmse: 0.8254 - lr: 0.0100\n",
      "Epoch 33/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1807 - mse: 0.2236 - mae: 0.3615 - mape: 78613.6328 - rmse: 0.4729\n",
      "Epoch 33: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1807 - mse: 0.2236 - mae: 0.3615 - mape: 78613.6328 - rmse: 0.4729 - val_loss: 0.3252 - val_mse: 0.6387 - val_mae: 0.6504 - val_mape: 60844.2656 - val_rmse: 0.7992 - lr: 0.0100\n",
      "Epoch 34/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1805 - mse: 0.2230 - mae: 0.3610 - mape: 80389.6406 - rmse: 0.4723\n",
      "Epoch 34: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1805 - mse: 0.2230 - mae: 0.3610 - mape: 80389.6406 - rmse: 0.4723 - val_loss: 0.3326 - val_mse: 0.6661 - val_mae: 0.6653 - val_mape: 67614.5781 - val_rmse: 0.8161 - lr: 0.0100\n",
      "Epoch 35/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1788 - mse: 0.2196 - mae: 0.3577 - mape: 74477.2266 - rmse: 0.4686\n",
      "Epoch 35: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 43ms/step - loss: 0.1788 - mse: 0.2196 - mae: 0.3577 - mape: 74477.2266 - rmse: 0.4686 - val_loss: 0.3479 - val_mse: 0.7175 - val_mae: 0.6958 - val_mape: 44074.3125 - val_rmse: 0.8470 - lr: 0.0100\n",
      "Epoch 36/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1778 - mse: 0.2177 - mae: 0.3556 - mape: 78811.4219 - rmse: 0.4666\n",
      "Epoch 36: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1778 - mse: 0.2177 - mae: 0.3556 - mape: 78811.4219 - rmse: 0.4666 - val_loss: 0.3308 - val_mse: 0.6672 - val_mae: 0.6617 - val_mape: 47189.5312 - val_rmse: 0.8168 - lr: 0.0100\n",
      "Epoch 37/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1774 - mse: 0.2170 - mae: 0.3548 - mape: 81338.3125 - rmse: 0.4658\n",
      "Epoch 37: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1774 - mse: 0.2170 - mae: 0.3548 - mape: 81338.3125 - rmse: 0.4658 - val_loss: 0.3428 - val_mse: 0.7118 - val_mae: 0.6856 - val_mape: 69280.6172 - val_rmse: 0.8437 - lr: 0.0100\n",
      "Epoch 38/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1762 - mse: 0.2145 - mae: 0.3524 - mape: 83427.1328 - rmse: 0.4631\n",
      "Epoch 38: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1762 - mse: 0.2145 - mae: 0.3524 - mape: 83427.1328 - rmse: 0.4631 - val_loss: 0.3231 - val_mse: 0.6385 - val_mae: 0.6462 - val_mape: 56298.6914 - val_rmse: 0.7991 - lr: 0.0100\n",
      "Epoch 39/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1753 - mse: 0.2126 - mae: 0.3505 - mape: 82811.4766 - rmse: 0.4611\n",
      "Epoch 39: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1753 - mse: 0.2126 - mae: 0.3505 - mape: 82811.4766 - rmse: 0.4611 - val_loss: 0.3332 - val_mse: 0.6644 - val_mae: 0.6663 - val_mape: 48565.7578 - val_rmse: 0.8151 - lr: 0.0100\n",
      "Epoch 40/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1740 - mse: 0.2101 - mae: 0.3480 - mape: 76456.8984 - rmse: 0.4584\n",
      "Epoch 40: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 43ms/step - loss: 0.1740 - mse: 0.2101 - mae: 0.3480 - mape: 76456.8984 - rmse: 0.4584 - val_loss: 0.3451 - val_mse: 0.7160 - val_mae: 0.6901 - val_mape: 51446.2383 - val_rmse: 0.8462 - lr: 0.0100\n",
      "Epoch 41/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1729 - mse: 0.2081 - mae: 0.3458 - mape: 81132.8984 - rmse: 0.4562\n",
      "Epoch 41: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 43ms/step - loss: 0.1729 - mse: 0.2081 - mae: 0.3458 - mape: 81132.8984 - rmse: 0.4562 - val_loss: 0.3373 - val_mse: 0.6914 - val_mae: 0.6746 - val_mape: 47320.0352 - val_rmse: 0.8315 - lr: 0.0100\n",
      "Epoch 42/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1725 - mse: 0.2068 - mae: 0.3449 - mape: 79318.1641 - rmse: 0.4547\n",
      "Epoch 42: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1725 - mse: 0.2068 - mae: 0.3449 - mape: 79318.1641 - rmse: 0.4547 - val_loss: 0.3429 - val_mse: 0.7090 - val_mae: 0.6858 - val_mape: 50945.7812 - val_rmse: 0.8420 - lr: 0.0100\n",
      "Epoch 43/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1708 - mse: 0.2037 - mae: 0.3416 - mape: 81279.6406 - rmse: 0.4514\n",
      "Epoch 43: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 45ms/step - loss: 0.1708 - mse: 0.2037 - mae: 0.3416 - mape: 81279.6406 - rmse: 0.4514 - val_loss: 0.3566 - val_mse: 0.7601 - val_mae: 0.7132 - val_mape: 47645.1445 - val_rmse: 0.8718 - lr: 0.0100\n",
      "Epoch 44/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1695 - mse: 0.2012 - mae: 0.3390 - mape: 78724.4922 - rmse: 0.4485\n",
      "Epoch 44: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1695 - mse: 0.2012 - mae: 0.3390 - mape: 78724.4922 - rmse: 0.4485 - val_loss: 0.3332 - val_mse: 0.6720 - val_mae: 0.6665 - val_mape: 53035.3281 - val_rmse: 0.8198 - lr: 0.0100\n",
      "Epoch 45/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1686 - mse: 0.1993 - mae: 0.3373 - mape: 78033.8828 - rmse: 0.4465\n",
      "Epoch 45: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.1686 - mse: 0.1993 - mae: 0.3373 - mape: 78033.8828 - rmse: 0.4465 - val_loss: 0.3319 - val_mse: 0.6666 - val_mae: 0.6639 - val_mape: 60172.3398 - val_rmse: 0.8164 - lr: 0.0100\n",
      "Epoch 46/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1678 - mse: 0.1975 - mae: 0.3356 - mape: 83683.8281 - rmse: 0.4444\n",
      "Epoch 46: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 51ms/step - loss: 0.1678 - mse: 0.1975 - mae: 0.3356 - mape: 83683.8281 - rmse: 0.4444 - val_loss: 0.3513 - val_mse: 0.7390 - val_mae: 0.7025 - val_mape: 46136.7344 - val_rmse: 0.8597 - lr: 0.0100\n",
      "Epoch 47/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1672 - mse: 0.1959 - mae: 0.3344 - mape: 79756.1484 - rmse: 0.4426\n",
      "Epoch 47: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 46ms/step - loss: 0.1672 - mse: 0.1959 - mae: 0.3344 - mape: 79756.1484 - rmse: 0.4426 - val_loss: 0.3342 - val_mse: 0.6713 - val_mae: 0.6685 - val_mape: 49133.6562 - val_rmse: 0.8193 - lr: 0.0100\n",
      "Epoch 48/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1651 - mse: 0.1919 - mae: 0.3303 - mape: 83784.0078 - rmse: 0.4380\n",
      "Epoch 48: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 47ms/step - loss: 0.1651 - mse: 0.1919 - mae: 0.3303 - mape: 83784.0078 - rmse: 0.4380 - val_loss: 0.3291 - val_mse: 0.6557 - val_mae: 0.6583 - val_mape: 55424.8047 - val_rmse: 0.8098 - lr: 0.0100\n",
      "Epoch 49/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1639 - mse: 0.1894 - mae: 0.3279 - mape: 84410.7109 - rmse: 0.4352\n",
      "Epoch 49: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 45ms/step - loss: 0.1639 - mse: 0.1894 - mae: 0.3279 - mape: 84410.7109 - rmse: 0.4352 - val_loss: 0.3473 - val_mse: 0.7220 - val_mae: 0.6945 - val_mape: 49599.3984 - val_rmse: 0.8497 - lr: 0.0100\n",
      "Epoch 50/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1628 - mse: 0.1870 - mae: 0.3256 - mape: 82202.2266 - rmse: 0.4325\n",
      "Epoch 50: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 50ms/step - loss: 0.1628 - mse: 0.1870 - mae: 0.3256 - mape: 82202.2266 - rmse: 0.4325 - val_loss: 0.3489 - val_mse: 0.7241 - val_mae: 0.6979 - val_mape: 54460.5664 - val_rmse: 0.8509 - lr: 0.0100\n",
      "Epoch 51/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1613 - mse: 0.1840 - mae: 0.3226 - mape: 90132.5234 - rmse: 0.4290\n",
      "Epoch 51: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 46ms/step - loss: 0.1613 - mse: 0.1840 - mae: 0.3226 - mape: 90132.5234 - rmse: 0.4290 - val_loss: 0.3593 - val_mse: 0.7526 - val_mae: 0.7187 - val_mape: 42598.3789 - val_rmse: 0.8675 - lr: 0.0100\n",
      "Epoch 52/100\n",
      "88/89 [============================>.] - ETA: 0s - loss: 0.1605 - mse: 0.1821 - mae: 0.3210 - mape: 79670.1484 - rmse: 0.4267\n",
      "Epoch 52: val_loss did not improve from 0.31695\n",
      "89/89 [==============================] - 4s 47ms/step - loss: 0.1605 - mse: 0.1821 - mae: 0.3210 - mape: 80560.9141 - rmse: 0.4267 - val_loss: 0.3470 - val_mse: 0.7283 - val_mae: 0.6941 - val_mape: 40641.8242 - val_rmse: 0.8534 - lr: 0.0100\n",
      "Epoch 53/100\n",
      "89/89 [==============================] - ETA: 0s - loss: 0.1588 - mse: 0.1788 - mae: 0.3176 - mape: 84683.5781 - rmse: 0.4228\n",
      "Epoch 53: val_loss did not improve from 0.31695\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "89/89 [==============================] - 4s 46ms/step - loss: 0.1588 - mse: 0.1788 - mae: 0.3176 - mape: 84683.5781 - rmse: 0.4228 - val_loss: 0.3615 - val_mse: 0.7850 - val_mae: 0.7230 - val_mape: 42169.3750 - val_rmse: 0.8860 - lr: 0.0100\n",
      "Epoch 53: early stopping\n",
      "time computation seconds:  213.58462166786194\n",
      "355/355 [==============================] - 2s 7ms/step - loss: 0.1806 - mse: 0.2259 - mae: 0.3612 - mape: 73490.6484 - rmse: 0.4753\n",
      "loss:  0.18060776591300964 MSE:  0.22594596445560455 MAE:  0.3612154424190521 RMSE:  73490.6484375 MAPE:  0.4753377437591553\n"
     ]
    }
   ],
   "source": [
    "df_start, cell_name= preprocess_data(\"D:\\\\KULIAH\\\\teep\\\\AI\\\\dataset\\\\08_01_2024\\\\CellReports.csv\")\n",
    "#now = datetime.datetime.now()\n",
    "timestamp = \"cekpoint\"\n",
    "for index in range(0,1):\n",
    "    print(index)\n",
    "\n",
    "    train_df = df_start[df_start['Viavi.Cell.Name'] == train_name_cells[index]]\n",
    "    train_df = train_df.loc[~train_df.index.duplicated()]\n",
    "    train_df=train_df.drop(columns=['Viavi.Cell.Name']).astype(float).copy()\n",
    "\n",
    "    val_df= df_start[df_start['Viavi.Cell.Name'] == test_name_cells[0]]\n",
    "    val_df=val_df.loc[~val_df.index.duplicated()]\n",
    "    val_df=val_df.drop(columns=['Viavi.Cell.Name']).astype(float).copy()\n",
    "\n",
    "    name_file='2hour_%s_%s.hdf5'%(timestamp, index+1)\n",
    "    name_file_before='2hour_%s_%s.hdf5'%(timestamp, index)\n",
    "    print(\"name_file: \", name_file)\n",
    "    print(\"name_file_before: \", name_file_before)\n",
    "    print(\"Cell Name: \", cell_name[index])\n",
    "    display(train_df)\n",
    "    running_program(train_df=train_df, val_df=val_df, index_cell=index, name_file=name_file, name_file_before=name_file_before)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
